<!DOCTYPE html>
<html lang="en">

<!-- Head tag (contains Google-Analytics、Baidu-Tongji)-->
<head>
  <!-- Google Analytics -->
  

  <!-- Baidu Tongji -->
  

  <!-- Baidu Push -->
  

  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>

  <meta name="google-site-verification" content="lxDfCplOZbIzjhG34NuQBgu2gdyRlAtMB4utP5AgEBc"/>
  <meta name="baidu-site-verification" content="PpzM9WxOJU"/>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="It&#39;s an IT blog for simslaber"/>
  <meta name="keyword" content="NTUST,SiMS Lab,IT Blog"/>
  <link rel="shortcut icon" href="/it-blog/img/avatar/ntust-simslab.png"/>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>

  
    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/it-blog/css/bootstrap.min.css"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/it-blog/css/beantech.min.css"/>

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/it-blog/css/highlight.css"/>
    <link rel="stylesheet" href="/it-blog/css/widget.css"/>
    <link rel="stylesheet" href="/it-blog/css/rocket.css"/>
    <link rel="stylesheet" href="/it-blog/css/signature.css"/>
    <link rel="stylesheet" href="/it-blog/css/catalog.css"/>
    <link rel="stylesheet" href="/it-blog/css/livemylife.css"/>

    

    
      <!-- top start (article top hot config) -->
      <link rel="stylesheet" href="/it-blog/css/top.css"/>
      <!-- top end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/it-blog/css/scroll.css"/>
      <!-- ThemeColor end -->
    

    

    
      <!-- Search start -->
      <link rel="stylesheet" href="/it-blog/css/search.css"/>
      <!-- Search end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/it-blog/css/themecolor.css"/>
      <!-- ThemeColor end -->
    

    

    
  

  <!-- Custom Fonts -->
  <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <!-- Hux change font-awesome CDN to qiniu -->
  <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <!-- Hux Delete, sad but pending in China <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'> <link
  href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/ css'> -->

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]-->

  <!-- ga & ba script hoook -->
  <link rel="canonical" href="https://ntust-sims-lab.github.io/it-blog/paper_review_mmnet/">
  <title>
    
      【論文研讀】MMNet - A Model-Based Multimodal Networkfor Human Action Recognition in RGB-D Videos - SiMS Lab | IT Blog
    
  </title>
<meta name="generator" content="Hexo 5.4.0"></head>


<!-- hack iOS CSS :active style -->

	<body ontouchstart="" class="body--light body--light">


		<!-- ThemeColor -->
		
		<!-- ThemeColor -->
<style type="text/css">
  .body--light {
    --light-mode: none;
    --dark-mode: block;
  }
  .body--dark {
    --light-mode: block;
    --dark-mode: none;
  }
  i.mdui-icon.material-icons.light-mode {
    display: var(--light-mode);
  }
  i.mdui-icon.material-icons.dark-mode {
    display: var(--dark-mode);
  }
</style>
<div class="toggle" onclick="document.body.classList.toggle('body--dark')">
  <i class="mdui-icon material-icons light-mode"></i>
  <i class="mdui-icon material-icons dark-mode"></i>
</div>
<script>
  //getCookieValue
  function getCookieValue(a) {
    var b = document.cookie.match('(^|[^;]+)\\s*' + a + '\\s*=\\s*([^;]+)');
    return b
      ? b.pop()
      : '';
  }
  let themeMode = 'light';
  if (getCookieValue('sb-color-mode') && (getCookieValue('sb-color-mode') !== themeMode)) {
    let dbody = document.body.classList;
    themeMode === 'dark' ? dbody.remove('body--dark') : dbody.add('body--dark');
  }

  //setCookieValue
  var toggleBtn = document.querySelector(".toggle");
  toggleBtn.addEventListener("click", function () {
    var e = document.body.classList.contains("body--dark");
    var cookieString = e
      ? "dark"
      : "light";
    var exp = new Date();
    exp.setTime(exp.getTime() + 3 * 24 * 60 * 60 * 1000); //3天过期
    document.cookie = "sb-color-mode=" + cookieString + ";expires=" + exp.toGMTString() + ";path=/";
  });
</script>

		

		<!-- Gitter -->
		

		<!-- Navigation (contains search)-->
		<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/it-blog/">SiMS Lab</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <!-- Known Issue, found by Hux: <nav>'s height woule be hold on by its content. so, when navbar scale out, the <nav> will cover tags. also mask any touch event of tags, unfortunately. -->
    <div id="huxblog_navbar">
      <div class="navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="/it-blog/">HOME</a>
          </li>

          
          
          
          
          <li>
            <a href="/it-blog/archive/">
              
              ARCHIVES
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/it-blog/categories/">
              
              CATEGORIES
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/it-blog/tags/">
              
              TAGS
              
              
            </a>
          </li>
          
          

          
          <li>
            <a class="popup-trigger">
              <span class="search-icon"></span>SEARCH</a>
          </li>
          

          <!-- LangSelect -->
          
        </ul>
      </div>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>
<!-- progress -->
<div id="progress">
  <div class="line" style="width: 0%;"></div>
</div>

<script>
  // Drop Bootstarp low-performance Navbar Use customize navbar with high-quality material design animation in high-perf jank-free CSS3 implementation
  var $body = document.body;
  var $toggle = document.querySelector('.navbar-toggle');
  var $navbar = document.querySelector('#huxblog_navbar');
  var $collapse = document.querySelector('.navbar-collapse');

  $toggle.addEventListener('click', handleMagic)

  function handleMagic(e) {
    if ($navbar.className.indexOf('in') > 0) {
      // CLOSE
      $navbar.className = " ";
      // wait until animation end.
      setTimeout(function() {
        // prevent frequently toggle
        if ($navbar.className.indexOf('in') < 0) {
          $collapse.style.height = "0px"
        }
      }, 400)
    } else {
      // OPEN
      $collapse.style.height = "auto"
      $navbar.className += " in";
    }
  }
</script>


		<!-- Post Header (contains intro-header、signature、wordcount、busuanzi、waveoverlay) -->
		<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->

  <style type="text/css">
    .body--light {
      /* intro-header */
      --intro-header-background-image-url-home: url('/it-blog/img/header_img/home.jpg');
      --intro-header-background-image-url-post: url('');
      --intro-header-background-image-url-page: url('/it-blog/img/header_img/archive_bg2.jpg');
    }
    .body--dark {
      --intro-header-background-image-url-home: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/it-blog/img/header_img/home.jpg');
      --intro-header-background-image-url-post: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('');
      --intro-header-background-image-url-page: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/it-blog/img/header_img/archive_bg2.jpg');
    }

    header.intro-header {
       /*post*/
        background-image: var(--intro-header-background-image-url-post);
        /* background-image: url(''); */
      
    }

    
  </style>





<header class="intro-header">
  <!-- Signature -->
  <div id="signature">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
          
          <div class="post-heading">
            <div class="tags">
              
            </div>
            <h1>【論文研讀】MMNet - A Model-Based Multimodal Networkfor Human Action Recognition in RGB-D Videos</h1>
            <h2 class="subheading"></h2>
            <span class="meta">
              Posted by Frances Kao on
              2023-08-22
            </span>


            
            <!-- WordCount start -->
            <div class="blank_box"></div>
            <span class="meta">
              Estimated Reading Time <span class="post-count">14</span> Minutes
            </span>
            <div class="blank_box"></div>
            <span class="meta">
              Words <span class="post-count">3.5k</span> In Total
            </span>
            <div class="blank_box"></div>
            <!-- WordCount end -->
            
            
            <!-- 不蒜子统计 start -->
            <span class="meta" id="busuanzi_container_page_pv">
              Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
            </span>
            <!-- 不蒜子统计 end -->
            


          </div>
          
        </div>
      </div>
    </div>
  </div>

  

</header>



		<!-- Main Content (Post contains
	Pager、
	tip、
	socialshare、
	gitalk、gitment、disqus-comment、
	Catalog、
	Sidebar、
	Featured-Tags、
	Friends Blog、
	anchorjs、
	) -->
		<!-- Modify by Yu-Hsuan Yen -->
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1 post-container">

        <h1><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9782511?casa_token=ZGe_P3bVP_8AAAAA:TRqz91pF4uPIwDLzFCz0pgQgennYjUJe67ml7-Nb5qTOeDT3ee4O4WDKw07YnBK6CB9Xqcs">MMNet: A Model-Based Multimodal Network for Human Action Recognition in RGB-D Videos</a></h1>
<p><a target="_blank" rel="noopener" href="https://hackmd.io/NsbTaEWHQH-UJm-heBaFkg"><img src="https://hackmd.io/NsbTaEWHQH-UJm-heBaFkg/badge" alt="hackmd-github-sync-badge"></a></p>
<ul>
<li>Journal reference: IEEE Transactions on Pattern Analysis and Machine Intelligence (VOL.45, NO.3, MARCH 2023)</li>
<li>Authors: Bruce X.B. Yu, Yan Liu, Xiang Zhang, Sheng-hua Zhong, and Keith C.C. Chan</li>
<li>Github: <a target="_blank" rel="noopener" href="https://github.com/bruceyo/MMNet">https://github.com/bruceyo/MMNet</a></li>
</ul>
<hr>
<h2 id="Introduction">Introduction</h2>
<p>本研究建構一RGB模態資料，其假設為「當有一定時序內提供足夠的空間和外觀特徵時，人類即可輕鬆識別動作。」以此前提，使機器模擬人類識別動作的方式。人類在識別動作時，也會同時關注與人互動的物件。也就是說，我們會關注人體動作的移動，以及物件的外觀特徵，並將其連結以得知該動作類別。因此，作者構建一時空感知區域 (spatiotemporal region of interest, ST-ROI) 之特徵圖，透過關注RGB影像中<strong>有變化的</strong>外觀特徵，以減少處理影片資料時的高運算量問題。<br><br>
然而，直接將直接將ST-ROI餵入這些VGG或ResNet等深度學習模型時，容易造成過擬合，而未能達到令人滿意的結果。因此，作者提出了一方法，透過骨架模態的知識轉移（Knowledge transferring），以提升RGB模態的動作辨識能力。具體而言，MMNet透過注意力遮罩的設計，針對可提供互補特徵的ST-ROI區域進行關注。</p>
<h3 id="Contributions">Contributions</h3>
<ol>
<li>提出了一種多模態深度學習架構，該架構在模型級別融合了不同的數據模態，並使用骨骼骨頭流(skeleton bone stream)和注意力機制，以提升RGB-D影片中的動作辨識。</li>
<li>在三個基準數據集（NTU RGB + D 120 [10]，PKU-MMD [23]和Northwestern-UCLA Multiview [24]）上展示了該方法的優異性能，大大提高了現有技術的水平。</li>
<li>通過分析所提出的MMNet的兩個關鍵參數，進一步驗證了該方法的有效性。</li>
</ol>
<h3 id="現有技術遇到的困難">現有技術遇到的困難</h3>
<ol>
<li>僅透過單模態資料作為人體動作辨識之輸入的結果過於狹隘，例如RGB-based方法欠缺3D結構資訊，易受到背景或角度影響；而skeleton-based則欠缺紋理和色彩資訊，使骨架動向相像的動作會難以辨別。如下圖，相似動作的骨架構成相同，僅透過骨架進行辨識將會忽略動作間的細微差異。<br>
<img src="https://hackmd.io/_uploads/HkaYxg9h2.png" alt=""></li>
<li>多項研究指出，使用多模態資料確實提升了HAR結果。然而，要如何有效地結合多模態資料以提升HAR之準確度，仍是個值得探討的議題。</li>
</ol>
<hr>
<h2 id="Related-Works">Related Works</h2>
<h3 id="Unimodal-HAR">Unimodal HAR</h3>
<ol>
<li>Skeleton-based HAR: 欠缺紋理和色彩資訊，因此骨架動向相像的動作會難以辨別。</li>
<li>RGB Video-based HAR: 欠缺3D結構資訊</li>
</ol>
<h3 id="Multimodal-HAR">Multimodal HAR</h3>
<p>其他多模態方法如: 結合骨架點位與其連接(joint &amp; bone)、不同感測器資料之結合</p>
<ol>
<li>Fusion-Based Multimodal HAR (data fusion[14])</li>
</ol>
<ul>
<li>可依據融合的資料分為以下三種：
<ul>
<li>data-level fusion: 在資料層次進行融合，將不同模態的資料進行結合，形成一個新的資料集。這種方法通常用於資料模態相同或相似的情況下，例如將RGB和HSV的圖像資料合併。</li>
<li>feature-level fusion: 在特徵層次進行融合，將不同模態的特徵進行結合，形成一個新的特徵集。這種方法通常用於資料模態不同但有相似特徵的情況下，例如將RGB圖像和深度圖像的特徵進行結合。常見方法為在全連接層進行特徵融合。</li>
<li>decision-level fusion: 在決策層次進行融合，將不同模態的決策結果進行結合，形成一個新的決策結果。這種方法通常用於資料模態不同且沒有相似特徵的情況下。常見方法為將softmax層得結果整合，例如將RGB影像與聲音資料的決策結果進行結合。</li>
</ul>
</li>
<li>亦可依據其表示方式分為以下兩種[18]：
<ul>
<li>joint representations：在特徵空間中將不同模態的特徵進行連接。</li>
<li>coordinated representations：將不同模態之間的表示進行對齊後，在進行連接。例如，將不同模態的特徵映射到同一個空間中，或者進行相關性分析。</li>
</ul>
</li>
</ul>
<blockquote>
<p>Our multimodal setting is distinct from that in [48], where not all data modalities were available in the testing phase. Instead, we focus on a case where all data modalities were available for the training and testing phases, leading to a multimodal learning setting called multimodal fusion [50].</p>
</blockquote>
<ol start="2">
<li>Model-Based Multimodal HAR</li>
</ol>
<ul>
<li>透過專注於對骨架模態帶來互補特徵的身體區域，從RGB模態中學習其表示(symbol)。即先提取不同模態之特徵表示後，再將其加以融合。</li>
<li>概念上像co-learning[18]，將不同模態之間的資訊互補或相互學習、融合，以提高模型的準確性。</li>
</ul>
<blockquote>
<p>Barsoum et al. [37] developed a sequence-to-sequence model for probabilistic human motion prediction, which predicts multiple plausible future human poses from the same input. However, it is not yet clear whether the generated data can be used to enhance HAR models’ generalization abilities or accuracy.</p>
</blockquote>
<hr>
<h2 id="Method">Method</h2>
<p><img src="https://hackmd.io/_uploads/ryAsVWqnn.png" alt=""></p>
<h3 id="MODEL-BASED-MULTIMODAL-NETWORK">MODEL-BASED MULTIMODAL NETWORK</h3>
<ul>
<li>模型輸入:
<ul>
<li>skeleton joint: 人體3D點位由深度相機(kinect)提供</li>
<li>skeleton bone: 由skeleton joint轉換而來</li>
<li>RGB video: 後續將對影像針對人體部位進行ROI裁切，但此處的人體骨架點位是另外由pose estimation model取得而非kinect</li>
</ul>
</li>
<li>模型輸出:<br>
$\hat{y} = G_J(O_J, J) + G_B(O_B, B) + G_V(O_V, V)$</li>
</ul>
<h4 id="ST-ROI-Construct-ST-ROI-From-RGB-Modality">ST-ROI (Construct ST-ROI From RGB Modality)</h4>
<ul>
<li>透過OpenPose取得人體骨架點位，再藉由transformation function $g$ 將點位轉換成ROI。其公式中的 $f^{(i)}_{t}$ 為影片 $V^i$ 的第 $i$ 幀，而 $o^{(i)}_{tj}$ 則是在第 $i$ 幀時OpenPose的第 $j$ 個點位。其中， $j$ 的數量不會超過OpenPose全部點位數量，僅取 $M’$ 個我們感興趣的點位： $$R^{(i)}_{tj} = g(f^{(i)}_{t}, o^{(i)}_{tj})$$</li>
<li>以時序採樣的方式，選取 $L$ 個代表幀，再將其垂直合併為 $R^{(i)_{t}}$ ；同樣地，於不同幀地不同骨架點位轉換為一正方形的ROI $R^{(i)}_{tj}$ 後，也會被水平地合併為 $R^{(i)_{j}}$ 。這兩組資料合併後即構成一幀方形的ST-ROI，其每一行代表一幀資訊（垂直），每一列代表一骨架資訊（水平）。因此，最後會以 $R^{i}$ 作為 $V^{(i)}$ 的代表，其中每一幀皆由 $M’ \times L$ 大小的 子ST-ROI $R^{(i)}_{tj}$ 作為代表。<br>
<img src="https://hackmd.io/_uploads/BkGVrz5hn.png" alt=""></li>
</ul>
<h4 id="Joint-Weights-Learn-Joint-Weights-From-Skeleton-Modality">Joint Weights (Learn Joint Weights From Skeleton Modality)</h4>
<ul>
<li>
<p>骨架的輸入如fig. 4所示，基本上與GCN模型相同，透過Graph Convolution整合了鄰近骨架點位的資訊（joint vertices），以計算特徵表示。這裡所得到的權重會根據joint vertice間的距離與joint vertice在圖中的級別位置而定。<br>
<img src="https://hackmd.io/_uploads/rJyj2rQp2.png" alt=""><br>
$\hat{J^{(i)}} = \sum \Lambda^{-1/2} A \Lambda ^{-1/2}f_{in}(J^{(i)}) W_k \odot M_{k}$　<br>
<br><br>
其中 $M_k$ 為attention map，用以表示每一vertex的重要性。而 $\hat{J}^{(i)}$ 為一形狀 $(c, t, M)$　的tensor（其維度分別輸出channel數、時序長度、vertice數量），用來推論動作類別；以外，也會被轉換為joint weight，提供RGB模態關注資訊，公式如下，即為一個代表Ｍ個不同骨架點位的權重矩陣：<br>
$w^{(i)} = \dfrac{1}{ct} \sum^{c}_{1} \sum^{t}_{1} \sqrt{(\hat{J}_{ct}^{(i)})^2}$</p>
</li>
<li>
<p>透過GCN模型進行骨架模態的關節權重(Joint Weights)學習，其設定同ST-GCN <code>[1][2]</code>。</p>
<blockquote>
<p>ST-GCN模型中，每個vertex都有一個特徵向量，以表示該關節(joint)在動作辨識任務中的重要性。因此，可透過分析節點，從而進行關節權重的學習。</p>
</blockquote>
</li>
</ul>
<h4 id="Model-Based-Fusion">Model-Based Fusion</h4>
<ul>
<li>作者提出一基於空間權重機制的RGB幀融合方法，使機器更好地關注有提供重要訊息的RGB特徵。</li>
<li>其他研究通常基於注意力機制的方法，從RGB模態本身計算出關注權重。而本研究作者則是使用從骨架模態得到的關節權重，與ST-ROI（自RGB模態取得）相乘後，再得到正規過後的RGB模態資訊。<br>
<img src="https://hackmd.io/_uploads/SkxKa3Qq3h.png" alt=""></li>
</ul>
<h4 id="Objective-Function">Objective Function</h4>
<p>根據skeleton joint, skeleton bone, 和RGB video input個別的預測結果，設計目標函數為三者的cross-entropy加總: $L = L_J(\hat{y}^J, y) + L_B(\hat{y}^B, y) + L_V(\hat{y}^V, y)$</p>
<ul>
<li>skeleton joint<br>
$\hat{y}^{j^{(i)}} = \sigma(G_J(O_J, J^{(i)}))$<br>
其中， $G_J$ 為GCN； $O_J$ 為此GCN模型的可學習參數; $J^{(i)}$ 為骨架點位輸入的資料。</li>
<li>skeleton bone<br>
即skeleton joint資料的一種變形，會將兩個joint vector連接起來。<br>
$\hat{y}^{B^{(i)}} = \sigma(G_B(O_B, B^{(i)}))$<br>
其中， $G_B$ 為GCN； $O_B$ 為此GCN模型的可學習參數; $B^{(i)}$ 為skeleton bone input的資料。
<blockquote>
<p>假設有兩個joint vector， $\upsilon_{t1} = (x_1, y_1, z_1)$ 和 $\upsilon_{t2} = (x_2, y_2, z_2)$ ，則bone vector為 $B_{t1} = \varepsilon_{t1} = \upsilon_{t1} - \upsilon_{t2} = (x_1 - x_2, y_1 - y_2, z_1 - z_3)$。</p>
</blockquote>
</li>
<li>RGB video input<br>
透過ResNet對ST-ROI提取特徵。<br>
$\hat{y}^{V^{(i)}} = \sigma(G_V(R’^{(i)}, O_V) + R’^{(i)})$<br>
其中，$G_V(R’^{(i)}, O_V)$ 為需要學習的殘差映射(residual mapping)，利用殘差連接的方式關注差異處，可減少運算量； $O_V$ 為ResNet層的可學習參數。</li>
</ul>
<p>以上的 $\sigma$ 表示一線性層，用來將模型的輸出形狀轉換為one hot表示。</p>
<h4 id="Training-and-Optimization">Training and Optimization</h4>
<ul>
<li>透過joint weight的原方法(從RGB獲取權重)與本研究提出的方法(從skeleton獲取權重)，以驗證MMNet的有效性。</li>
<li>skeleton joint子模塊 $G_J$
<ul>
<li>$O_J$ 可先自行訓練並固定權重(pretrained and then fixed)，再進行 RGB vedio子模塊 $G_V$ 的訓練( $O_V$ )；</li>
<li>亦可 $O_J$ 和 $O_V$ 兩者同時訓練。</li>
</ul>
</li>
<li>skeleton bone子模塊 $G_B$ 則是完全分開訓練。</li>
<li>MMNet的優化流程如下圖表示:
<ol>
<li>輸入skeleton joint $J$ 訓練子模型 $G_J$ , 並且獲得joint weights $w$</li>
<li>透過RGB video $V$ 建構 $M’ \times L$ 大小的ST-ROI $R$</li>
<li>透過 $w$ 和 $R$ 建構skeleton-focused ST-ROI $R’$</li>
<li>輸入 $R’$ 訓練子模型 $G_V$</li>
<li>輸入 skeleton bone $B$ 訓練子模型 $G_B$</li>
<li>將三個子模型的預測結果合併<br>
<img src="https://hackmd.io/_uploads/HyEEw9g6h.png" alt=""></li>
</ol>
</li>
</ul>
<blockquote>
<p>Several other loss terms could be adopted for joint weights to pursue high recognition accuracy. For instance, according to the findings in [44], both the loss that encourages joint weights to maintain diversity and the loss that leads to joint weights with temporal variance can elicit slight recognition improvements.</p>
</blockquote>
<hr>
<h2 id="Results">Results</h2>
<h3 id="Datasets">Datasets</h3>
<ul>
<li>NTU RGB+D 60 (96 pixels)</li>
<li>NTU RGB+D 120 (96 pixels)</li>
<li>PKU-MMD (96 pixels)</li>
<li>Northwestern-UCLA (48 pixels)</li>
<li>Toyota Smarthome (48 pixels)</li>
</ul>
<h3 id="Implemetation-Details">Implemetation Details</h3>
<ul>
<li>ST-ROI: $M’ \times L$ 的 ST-ROI會先resize為225x225並且正歸化後，才會餵入ResNet18。而Northwestern-UCLA 和 Toyota Smarthome資料相對較小，因此會另外執行隨機翻轉的處理。</li>
<li>$G_J$ : 1) 使用了GCN模型計算空間權重(spatial weight) [1]; 2) 為了減輕時間位置(mean values of temporal positions)對於關節權重的影響，作者選擇了前15個權重值最大的時間位置來計算關節權重</li>
<li>optimizer: SGD (learning rate=0.1; 10th epoch=0.01; 50th epoch=0.001)</li>
<li>early stop: 80th epoch</li>
<li>minibatch size: 64</li>
<li>GPU: 4 GTX 1080 Ti GPUs</li>
</ul>
<h3 id="Experiments">Experiments</h3>
<h4 id="RGB-D-60">RGB+D 60</h4>
<ul>
<li>針對RGB Video融合上，做了權重更新的ablation study如下：
<ul>
<li>without joint weight：不使用skeleton modality學習來的權重，也就是在 $G_V$ 中沒有學習骨架資訊</li>
<li>dynamic joint weight：骨架子模組 $G_J$ 和RGB子模塊 $G_V$ (ResNet18)同時訓練，即$O_J$ 和 $O_V$ 會一起變動</li>
<li>fixed joint weight：訓練骨架子模組 $G_J$ 後固定權重( $O_J$ )(pretrained “GCN-Joints”)，再進行RGB子模塊 $G_V$ (ResNet18)的訓練( $O_V$ )</li>
</ul>
</li>
<li>除了以ResNet作為ST-ROI特徵提取，作者也嘗試Inception-v3和EfficientNet等backbone，並且取得更優異的表現。</li>
<li>相較於VPN++，雖然MMNet在此資料集的表現較為遜色，但在其他大型資料集如RGB+D 120或Northwestern-UCLA Multiview都超過其表現。<br>
<img src="https://hackmd.io/_uploads/SyiOccepn.png" alt=""><br>
<img src="https://hackmd.io/_uploads/S1EdLsla3.png" alt=""><br>
<img src="https://hackmd.io/_uploads/BySR8sl6h.png" alt=""><br>
<img src="https://hackmd.io/_uploads/BkStUjean.png" alt=""></li>
</ul>
<h4 id="RGB-D-120">RGB+D 120</h4>
<p><img src="https://hackmd.io/_uploads/SyRq8ixT3.png" alt=""></p>
<ul>
<li>比較了MS-G3D加入ST-ROI(fixed weights)的表現<br>
<img src="https://hackmd.io/_uploads/H1-9IjgT2.png" alt=""></li>
<li>特別與VPN進行比較，可以明顯看出在運行時間與資源占用上，MMNet是較為出色的。<br>
<img src="https://hackmd.io/_uploads/r1ssIiga2.png" alt=""><br>
<img src="https://hackmd.io/_uploads/Skivb3e63.png" alt=""></li>
</ul>
<blockquote>
<p>其他資料集的實驗結果大同小異，這邊不加以探討</p>
</blockquote>
<h4 id="Analysis-of-Joint-Weights">Analysis of Joint Weights</h4>
<ul>
<li>$G_J$ 可學習到骨架點位於特定時間點的重要性，顯現出一動作區間中骨架資訊會隨時間改變。若我們利用<a href="#Joint-Weights-(Learn-Joint-Weights-From-Skeleton-Modality)">joint weights公式</a>將所有時序位置資訊直接取平均值來獲取權重，便會失去特定重要時序上的特徵。</li>
<li>因此作者打算取前N個最重要的時序位置來計算joint weights，但作者發現在同個時序位置上的RGB資訊和骨架資訊並非同等重要，導致直接從兩模態融合的資訊來取前N個重要時序位置的資訊，效果並不好。</li>
<li>作者決定改以在每一人體部位( $J^{(i)}_{tj}$ )都取前15個最重要的時序位置，(即神經元激活值(neuron activation values)最大的前15個時序位置，他們代表了動作辨識中資訊的重要程度，用來計算其所對應身體部位的joint weights)。</li>
</ul>
<p><img src="https://hackmd.io/_uploads/HkO143x63.png" alt=""><br>
<img src="https://hackmd.io/_uploads/Sk_e43l6h.png" alt=""></p>
<h4 id="Analysis-of-Skeleton-Focused-Representation">Analysis of Skeleton-Focused Representation</h4>
<p><img src="https://hackmd.io/_uploads/BycW4ngpn.png" alt=""><br>
<img src="https://hackmd.io/_uploads/SkGPwng62.png" alt=""></p>
<hr>
<h2 id="Conclusion">Conclusion</h2>
<h3 id="Future-Works">Future Works</h3>
<ul>
<li>其他模態之研究: depth, optical flow</li>
<li>outdoor action: 考慮背景資訊, 以符合真實場景應用</li>
</ul>
<hr>
<h2 id="Discussion">Discussion</h2>
<ul>
<li>因RGB模態的模型需要待skeleton joint模態的模型訓練後得到joint weight參數才可繼續訓練，若要應用在即時且實際場景的辨識上將是一挑戰。</li>
<li>在RGB模態中，須額外使用姿態辨識模型推論骨架，而不是直接使用kinect所提供的骨架資訊，使方法複雜化。</li>
<li>若將skeleton input的資訊3D改為2D，是否會有明顯差異？若能調整為僅使用2D相機並且透過姿態辨識模型推論骨架，亦可較好將模型進行整合。</li>
<li>在此方法中，三種模態的資料需要各自進行模型訓練，再以essemble方式輸出辨識結果，可能相當耗時。是否為每種動作皆需要三種模態資料進行辨識, 可再加以探討。</li>
</ul>


        <hr>
        <!-- Pager -->
        <ul class="pager">
          
          <li class="previous">
            <a href="/it-blog/Simslab-server_guide/" data-toggle="tooltip" data-placement="top" title="【實驗室資源】Simslab-server使用指南">&larr; Previous Post</a>
          </li>
          
          
          <li class="next">
            <a href="/it-blog/APSNet_paper_review/" data-toggle="tooltip" data-placement="top" title="【論文研讀】APSNet - Toward Adaptive Point Sampling">Next Post &rarr;</a>
          </li>
          
        </ul>

        
        <!-- tip start -->
        <!-- tip -->
<!-- tip start -->
<div class="tip">
  <p>
    
       
    
  </p>
</div>
<!-- tip end -->

        <!-- tip end -->
        

        
        <!-- Sharing Srtart -->
        <!-- Social Social Share Post -->
<!-- Docs:https://github.com/overtrue/share.js -->

<div class="social-share" data-initialized="true" data-disabled="tencent ,douban ,qzone ,linkedin ,facebook ,google ,diandian" data-wechat-qrcode-helper="" align="center">
  <ul class="list-inline text-center social-share-ul">
    <!-- <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-twitter">
        <i class="fa fa-twitter fa-1x" aria-hidden="true"></i>
      </a>
    </li> -->
    <!-- <li class="social-share-li">
      <a class="social-share-icon icon-wechat">
        <i class="fa fa-weixin fa-1x" aria-hidden="true"></i>
      </a>
    </li> -->
    <!-- <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-weibo">
        <i class="fa fa-weibo fa-1x" aria-hidden="true"></i>
      </a>
    </li> -->
    <!-- <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-qq">
        <i class="fa fa-qq fa-1x" aria-hidden="true"></i>
      </a>
    </li> -->
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon" href="mailto:?subject=【論文研讀】MMNet - A Model-Based Multimodal Networkfor Human Action Recognition in RGB-D Videos&body=Hi,I found this website and thought you might like it https://ntust-sims-lab.github.io/it-blog/paper_review_mmnet/">
        <i class="fa fa-envelope fa-1x" aria-hidden="true"></i>
      </a>
    </li>
  </ul>
</div>

<!-- css & js -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"> -->
<script defer="defer" async="true" src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

        <!-- Sharing End -->
        
        <hr>

        <!-- comments start -->
        <!-- 1. gitalk comment -->


<!-- 2. gitment comment -->


<!-- 3. disqus comment -->


        <!-- comments end -->
        <hr>

      </div>

      <!-- Catalog: Tabe of Content -->
      <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">MMNet: A Model-Based Multimodal Network for Human Action Recognition in RGB-D Videos</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Introduction"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">Introduction</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Contributions"><span class="toc-nav-number">1.1.1.</span> <span class="toc-nav-text">Contributions</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%8F%BE%E6%9C%89%E6%8A%80%E8%A1%93%E9%81%87%E5%88%B0%E7%9A%84%E5%9B%B0%E9%9B%A3"><span class="toc-nav-number">1.1.2.</span> <span class="toc-nav-text">現有技術遇到的困難</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Related-Works"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">Related Works</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Unimodal-HAR"><span class="toc-nav-number">1.2.1.</span> <span class="toc-nav-text">Unimodal HAR</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Multimodal-HAR"><span class="toc-nav-number">1.2.2.</span> <span class="toc-nav-text">Multimodal HAR</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Method"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">Method</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#MODEL-BASED-MULTIMODAL-NETWORK"><span class="toc-nav-number">1.3.1.</span> <span class="toc-nav-text">MODEL-BASED MULTIMODAL NETWORK</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ST-ROI-Construct-ST-ROI-From-RGB-Modality"><span class="toc-nav-number">1.3.1.1.</span> <span class="toc-nav-text">ST-ROI (Construct ST-ROI From RGB Modality)</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Joint-Weights-Learn-Joint-Weights-From-Skeleton-Modality"><span class="toc-nav-number">1.3.1.2.</span> <span class="toc-nav-text">Joint Weights (Learn Joint Weights From Skeleton Modality)</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Model-Based-Fusion"><span class="toc-nav-number">1.3.1.3.</span> <span class="toc-nav-text">Model-Based Fusion</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Objective-Function"><span class="toc-nav-number">1.3.1.4.</span> <span class="toc-nav-text">Objective Function</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Training-and-Optimization"><span class="toc-nav-number">1.3.1.5.</span> <span class="toc-nav-text">Training and Optimization</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Results"><span class="toc-nav-number">1.4.</span> <span class="toc-nav-text">Results</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Datasets"><span class="toc-nav-number">1.4.1.</span> <span class="toc-nav-text">Datasets</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Implemetation-Details"><span class="toc-nav-number">1.4.2.</span> <span class="toc-nav-text">Implemetation Details</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Experiments"><span class="toc-nav-number">1.4.3.</span> <span class="toc-nav-text">Experiments</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#RGB-D-60"><span class="toc-nav-number">1.4.3.1.</span> <span class="toc-nav-text">RGB+D 60</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#RGB-D-120"><span class="toc-nav-number">1.4.3.2.</span> <span class="toc-nav-text">RGB+D 120</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Analysis-of-Joint-Weights"><span class="toc-nav-number">1.4.3.3.</span> <span class="toc-nav-text">Analysis of Joint Weights</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Analysis-of-Skeleton-Focused-Representation"><span class="toc-nav-number">1.4.3.4.</span> <span class="toc-nav-text">Analysis of Skeleton-Focused Representation</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Conclusion"><span class="toc-nav-number">1.5.</span> <span class="toc-nav-text">Conclusion</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Future-Works"><span class="toc-nav-number">1.5.1.</span> <span class="toc-nav-text">Future Works</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Discussion"><span class="toc-nav-number">1.6.</span> <span class="toc-nav-text">Discussion</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    



      <!-- Sidebar Container -->
      <div class="
                col-lg-8 col-lg-offset-1
                col-md-10 col-md-offset-1
                sidebar-container">

        <!-- Featured Tags -->
        

        <!-- Friends Blog -->
        
      </div>
    </div>
  </div>
</article>



<!-- anchorjs start -->
<!-- async load function -->
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script type="text/javascript">
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function(e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  };
</script>
<script type="text/javascript">
  //anchor-js, Doc:http://bryanbraun.github.io/anchorjs/
  async ("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function() {
    anchors.options = {
      visible: 'hover',
      placement: 'left',
      // icon: 'ℬ'
      icon: '❡'
    };
    anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
  });
</script>
<style>
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>

<!-- anchorjs end -->



		<!-- Footer (contains ThemeColor、viewer) -->
		<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center">
          

          
            <li>
              <a target="_blank" href="https://github.com/NTUST-SiMS-Lab">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          

          

          

          

          

          

          

        </ul>
        <p class="copyright text-muted">
          Copyright &copy;
          @ntust-simslab
          2023
          <br>
          Theme by
          <a target="_blank" rel="noopener" href="http://beantech.org">BeanTech</a>
          <span style="display: inline-block; margin: 0 5px;">
            <i class="fa fa-heart"></i>
          </span>
          re-Ported by
          <a target="_blank" rel="noopener" href="https://v-vincen.life/">Live My Life</a>
          |
          <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=V-Vincen&repo=V-Vincen.github.io&type=star&count=true"></iframe>
        </p>
      </div>
    </div>
  </div>
</footer>

<a id="rocket" href="#top" class=""></a>


  <!-- jQuery -->
  <script type="text/javascript" src="/it-blog/js/jquery.min.js"></script>
  <!-- Bootstrap Core JavaScript -->
  <script type="text/javascript" src="/it-blog/js/bootstrap.min.js"></script>
  <!-- Custom Theme JavaScript -->
  <script type="text/javascript" src="/it-blog/js/hux-blog.min.js"></script>
  <!-- catalog -->
  <script async="true" type="text/javascript" src="/js/catalog.js"></script>
  <!-- totop(rocket) -->
  <script async="true" type="text/javascript" src="/js/totop.js"></script>

  
    <!-- Busuanzi JavaScript -->
    <script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <!-- Scroll start -->
    <script async="async" type="text/javascript" src="/it-blog/js/scroll.js"></script>
    <!-- Scroll end -->
  

  

  

  

  







<script>
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function (e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  }

  // fastClick.js
  async ("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function () {
    var $nav = document.querySelector("nav");
    if ($nav)
      FastClick.attach($nav);
    }
  )
</script>

<!-- Because of the native support for backtick-style fenced code blocks right within the Markdown is landed in Github Pages, From V1.6, There is no need for Highlight.js, so Huxblog drops it officially. -
https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0 - https://help.github.com/articles/creating-and-highlighting-code-blocks/ -->
<!-- <script> async ("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function () { hljs.initHighlightingOnLoad(); }) </script> <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet"> -->

<!-- jquery.tagcloud.js -->
<!-- <script> // only load tagcloud.js in tag.html if ($('#tag_cloud').length !== 0) { async ("https://ntust-sims-lab.github.io/it-blog/js/jquery.tagcloud.js", function () { $.fn.tagcloud.defaults = { // size: { start: 1, end: 1, unit: 'em' }, color: {
start: '#bbbbee', end: '#0085a1' } }; $('#tag_cloud a').tagcloud(); }) } </script> -->


		<!-- Search -->
		
		<div class="popup search-popup local-search-popup">
  <span class="popup-btn-close">
    ESC
  </span>
  <div class="container">
    <div class="row">
      <!-- <div class="col-md-9 col-md-offset-1"> -->
      <div class="col-lg-9 col-lg-offset-1 col-md-10 col-md-offset-1 local-search-content">

        <div class="local-search-header clearfix">

          <div class="local-search-input-wrapper">
            <span class="search-icon">
              <i class="fa fa-search fa-lg" style="margin: 25px 10px 25px 20px;"></i>
            </span>
            <input autocomplete="off" placeholder="SEARCH..." type="text" id="local-search-input">
          </div>
        </div>
        <div id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>


  
    <script src="/it-blog/js/ziploader.js"></script>
  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    // monitor main search box;
    var onPopupClose = function (e) {
      $('.popup').fadeOut(300);
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $('.popup').fadeIn(300);
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }
    // get search zip version
    $.get('/it-blog/searchVersion.json?t=' + (+new Date()), function (res) {
      if (localStorage.getItem('searchVersion') !== res) {
        localStorage.setItem('searchVersion', res);
        initSearchJson();
      }
    });

    function initSearchJson() {
      initLoad(['/it-blog/search.flv'], {
        loadOptions: {
          success: function (obj) {
            localStorage.setItem('searchJson', obj['search.json'])
          },
          error: function (e) {
            return console.log(e)
          }
        },
        returnOptions: {
          'json': TYPE_TEXT
        },
        mimeOptions: {
          'json': 'application/json'
        }
      })
    }
    // search function;
    var searchFunc = function (search_id, content_id) {
      'use strict';
      isfetched = true;
      var datas = JSON.parse(localStorage.getItem('searchJson'));
      // console.log(search_id)
      var input = document.getElementById(search_id);
      var resultContent = document.getElementById(content_id);
      var inputEventFunction = function () {
        var searchText = input.value.trim().toLowerCase();
        var keywords = searchText.split(/[\s\-]+/);
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        var resultItems = [];
        if (searchText.length > 0) {
          // perform local searching
          datas.forEach(function (data) {
            var isMatch = false;
            var hitCount = 0;
            var searchTextCount = 0;
            var title = data.title
              ? data.title.trim()
              : '';
            var titleInLowerCase = title.toLowerCase();
            var content = data.content
              ? data.content.trim().replace(/<[^>]+>/g, "")
              : '';
            var contentInLowerCase = content.toLowerCase();
            var articleUrl = decodeURIComponent(data.url);

            var date = data.date;
            var dateTime = date.replace(/T/, " ").replace(/.000Z/, "");
            var imgUrl = data.header_img;
            


            var indexOfTitle = [];
            var indexOfContent = [];
            // only match articles with not empty titles
            keywords.forEach(function (keyword) {
              function getIndexByWord(word, text, caseSensitive) {
                var wordLen = word.length;
                if (wordLen === 0) {
                  return [];
                }
                var startPosition = 0,
                  position = [],
                  index = [];
                if (!caseSensitive) {
                  text = text.toLowerCase();
                  word = word.toLowerCase();
                }
                while ((position = text.indexOf(word, startPosition)) > -1) {
                  index.push({position: position, word: word});
                  startPosition = position + wordLen;
                }
                return index;
              }
              indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
              indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
            });
            if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
              isMatch = true;
              hitCount = indexOfTitle.length + indexOfContent.length;
            }
            // show search results
            if (isMatch) {
              // sort index by position of keyword
              [indexOfTitle, indexOfContent].forEach(function (index) {
                index.sort(function (itemLeft, itemRight) {
                  if (itemRight.position !== itemLeft.position) {
                    return itemRight.position - itemLeft.position;
                  } else {
                    return itemLeft.word.length - itemRight.word.length;
                  }
                });
              });
              // merge hits into slices
              function mergeIntoSlice(text, start, end, index) {
                var item = index[index.length - 1];
                var position = item.position;
                var word = item.word;
                var hits = [];
                var searchTextCountInSlice = 0;
                while (position + word.length <= end && index.length != 0) {
                  if (word === searchText) {
                    searchTextCountInSlice++;
                  }
                  hits.push({position: position, length: word.length});
                  var wordEnd = position + word.length;
                  // move to next position of hit
                  index.pop();
                  while (index.length != 0) {
                    item = index[index.length - 1];
                    position = item.position;
                    word = item.word;
                    if (wordEnd > position) {
                      index.pop();
                    } else {
                      break;
                    }
                  }
                }
                searchTextCount += searchTextCountInSlice;
                return {hits: hits, start: start, end: end, searchTextCount: searchTextCountInSlice};
              }
              var slicesOfTitle = [];
              if (indexOfTitle.length != 0) {
                slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
              }
              var slicesOfContent = [];
              while (indexOfContent.length != 0) {
                var item = indexOfContent[indexOfContent.length - 1];
                var position = item.position;
                var word = item.word;
                // cut out 100 characters
                var start = position - 20;
                var end = position + 80;
                if (start < 0) {
                  start = 0;
                }
                if (end < position + word.length) {
                  end = position + word.length;
                }
                if (end > content.length) {
                  end = content.length;
                }
                slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
              }
              // sort slices in content by search text's count and hits' count
              slicesOfContent.sort(function (sliceLeft, sliceRight) {
                if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                  return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                  return sliceRight.hits.length - sliceLeft.hits.length;
                } else {
                  return sliceLeft.start - sliceRight.start;
                }
              });
              // select top N slices in content
              var upperBound = parseInt('1');
              if (upperBound >= 0) {
                slicesOfContent = slicesOfContent.slice(0, upperBound);
              }
              // highlight title and content
              function highlightKeyword(text, slice) {
                var result = '';
                var prevEnd = slice.start;
                slice.hits.forEach(function (hit) {
                  result += text.substring(prevEnd, hit.position);
                  var end = hit.position + hit.length;
                  result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                  prevEnd = end;
                });
                result += text.substring(prevEnd, slice.end);
                return result;
              }
              var resultItem = '';

              // if (slicesOfTitle.length != 0) {   resultItem += "<li><a target='_blank' href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>"; } else {   resultItem += "<li><a target='_blank' href='" +
              // articleUrl + "' class='search-result-title'>" + title + "</a>"; } slicesOfContent.forEach(function (slice) {   resultItem += "<a target='_blank' href='" + articleUrl + "'><p class=\"search-result\">" + highlightKeyword(content, slice) +
              // "...</p></a>"; }); resultItem += "</li>";

              if (slicesOfTitle.length != 0) {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</div><time class='search-result-date'>" + dateTime + "</time>";
              } else {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + title + "</div><time class='search-result-date'>" + dateTime + "</time>";
              }
              slicesOfContent.forEach(function (slice) {
                resultItem += "<p class=\"search-result-content\">" + highlightKeyword(content, slice) + "...</p>";
              });
              resultItem += "</div><div class='search-result-right'><img class='media-image' src='" + imgUrl + "' width='64px' height='48px'></img></div></a>";

              resultItems.push({item: resultItem, searchTextCount: searchTextCount, hitCount: hitCount, id: resultItems.length});
            }
          })
        };

        if (keywords.length === 1 && keywords[0] === "") {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else if (resultItems.length === 0) {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else {
          resultItems.sort(function (resultLeft, resultRight) {
            if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
              return resultRight.searchTextCount - resultLeft.searchTextCount;
            } else if (resultLeft.hitCount !== resultRight.hitCount) {
              return resultRight.hitCount - resultLeft.hitCount;
            } else {
              return resultRight.id - resultLeft.id;
            }
          });
          var searchResultList = '<div class=\"search-result-list\">';
          resultItems.forEach(function (result) {
            searchResultList += result.item;
          })
          searchResultList += "</div>";
          resultContent.innerHTML = searchResultList;
        }
      }
      if ('auto' === 'auto') {
        input.addEventListener('input', inputEventFunction);
      } else {
        $('.search-icon').click(inputEventFunction);
        input.addEventListener('keypress', function (event) {
          if (event.keyCode === 13) {
            inputEventFunction();
          }
        });
      }
      // remove loading animation
      $('body').css('overflow', '');
      proceedsearch();
    }
    // handle and trigger popup window;
    $('.popup-trigger').click(function (e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc('local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });
    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function (e) {
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 && $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });

    document.addEventListener('mouseup', (e) => {
      var _con = document.querySelector(".local-search-content");
      if (_con) {
        if (!_con.contains(e.target)) {
          onPopupClose();
        }
      }
    });
  </script>


		
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
